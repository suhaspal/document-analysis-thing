{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import re\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import fitz\n",
    "from docx.document import Document\n",
    "from docx.oxml.shape import CT_Picture\n",
    "from docx.parts.image import ImagePart\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.replace('\\u00a0', ' ')\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    return text\n",
    "\n",
    "def perform_ocr(image):\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    text = ''\n",
    "    with fitz.open(file_path) as pdf:\n",
    "        for page in pdf:\n",
    "            text += page.get_text()\n",
    "            \n",
    "            for img in page.get_images():\n",
    "                xref = img[0]\n",
    "                base_image = pdf.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                text += ' ' + perform_ocr(image)\n",
    "    \n",
    "    return clean_text(text)\n",
    "\n",
    "def extract_docx_text(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "    \n",
    "    for rel in doc.part.rels.values():\n",
    "        if isinstance(rel._target, ImagePart):\n",
    "            image_bytes = rel._target.blob\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            text += ' ' + perform_ocr(image)\n",
    "    \n",
    "    return clean_text(text)\n",
    "\n",
    "def extract_txt_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return clean_text(file.read())\n",
    "\n",
    "def extract_image_text(file_path):\n",
    "    image = Image.open(file_path)\n",
    "    text = perform_ocr(image)\n",
    "    return clean_text(text)\n",
    "\n",
    "def upload_and_parse_document(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if file_extension == '.pdf':\n",
    "        return extract_pdf_text(file_path)\n",
    "    elif file_extension in ['.docx', '.doc']:\n",
    "        return extract_docx_text(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        return extract_txt_text(file_path)\n",
    "    elif file_extension in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']:\n",
    "        return extract_image_text(file_path)\n",
    "    else:\n",
    "        raise ValueError('Unsupported file type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\suhrp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "def vectorize_text(text, word2vec_model):\n",
    "    tokens = tokenize_text(text)\n",
    "    word_vectors = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "\n",
    "def process_document_vector(file_path):\n",
    "    text = upload_and_parse_document(file_path)\n",
    "    \n",
    "    tokenized_text = [tokenize_text(text)]\n",
    "    \n",
    "    word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    text_vector = vectorize_text(text, word2vec_model)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "    \n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_word_vectors = []\n",
    "    \n",
    "    for word in tfidf_feature_names:\n",
    "        if word in word2vec_model.wv:\n",
    "            word_vector = word2vec_model.wv[word]\n",
    "            tfidf_score = tfidf_matrix[0, tfidf_vectorizer.vocabulary_[word]]\n",
    "            tfidf_word_vectors.append(word_vector * tfidf_score)\n",
    "    \n",
    "    if tfidf_word_vectors:\n",
    "        final_vector = np.mean(tfidf_word_vectors, axis=0)\n",
    "    else:\n",
    "        final_vector = np.zeros(word2vec_model.vector_size)\n",
    "    \n",
    "    return final_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suhrp\\Downloads\\document-analysis-thing\\final-task\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in GPT4AllEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def get_vector_db(file_path):\n",
    "    text = upload_and_parse_document(\"Pre-Implementation Design Report.pdf\")\n",
    "    embeddings = GPT4AllEmbeddings()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    persist_directory = \"./chroma_db\"\n",
    "    vectordb = Chroma.from_texts(\n",
    "        texts=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_pipeline(file_path):\n",
    "    text = upload_and_parse_document(file_path)\n",
    "    final_vec = process_document_vector(file_path)\n",
    "    vector_db = get_vector_db(file_path)\n",
    "    return text, final_vec, vector_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation, final_vector, vector_chroma = document_pipeline(\"Pre-Implementation Design Report.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Overview Develop a prototype for an advanced document analysis system using transformer-based models, incorporating real-time annotation capabilities. This task is designed to be completed in 8-10 hours and should showcase your team's ability to work with state-of-the-art NLP models, handle data processing, and create an interactive user interface. Project Goals 1. Implement a document ingestion and preprocessing pipeline ● Take pdf and store in mongo db ● Upload newly annotated documents to mongo db upon finishing annotation, for the sake of version history and referencing ● Create one pipeline function which will accept all the text in the initial file and then repeatedly accept new text as real time annotation happens ○ Create a function that accepts a file path and set up conditionals to check what the file extension is. ■ Use PyPDF2’s PdfReader module to iterate through pdf documents and use the extract text built in function ■ Use docx module to handle situations where the document is a docx or doc file ■ Can handle text files without external modules in in python ■ Remove metadata and filter using PyPDF 2 ■ Have clean text ready for distil bert model ○ Word2vec for vectorization ○ Tfidf for text splitting ○ Use nomic for embeddings ○ Add to Pinecone Vector DB 2. Utilize transformer models for advanced text analysis and embedding ● Use Distil-BERT, fine tune two separate models, one for NER and one for relation extraction 3. Develop a sophisticated named entity recognition and relation extraction system ● Call NER pipeline to extract unique entities, and use relation extraction pipeline to analyze relation between every unique pair of entities that were gathered. 4. Create an interactive web interface with real-time annotation capabilities ● Four primary components: ○ Document Upload, once uploaded you can toggle between extracted text and the actual document ○ Google docs type annotation section ○ Search box to utilize rag in order to bring up information within the document, retrieved info section ○ Generated AI info section, result of NER and relation extraction pipeline 5. Implement a basic versioning system for annotations ● Mongo DB version control\n",
      "[-3.10504802e-05  3.05246031e-05  9.47373064e-06  2.01575785e-05\n",
      " -1.70049407e-05 -1.02000020e-04  3.17785998e-05  1.11032008e-04\n",
      " -4.90171515e-05 -7.86067249e-05  4.10015055e-05 -5.27673874e-05\n",
      " -2.29856269e-05  2.80200111e-05 -4.37785957e-06  1.11329009e-05\n",
      "  5.53917744e-05 -5.13098712e-05 -3.16368642e-05 -9.92436035e-05\n",
      "  1.73455446e-05  6.99207339e-06  5.62711321e-05 -1.49075286e-05\n",
      " -2.90651788e-05  3.03965180e-06 -2.81285975e-05 -2.29910474e-05\n",
      " -1.22785841e-05 -5.57289741e-06  3.26634436e-05 -2.60854995e-06\n",
      "  1.73487497e-05 -4.23439451e-05 -2.64448445e-05  8.41519650e-05\n",
      "  5.27095290e-05 -2.19667454e-05 -2.37079166e-05 -3.25517358e-05\n",
      "  1.18275821e-05 -5.81174318e-05 -2.09305235e-05 -1.16574993e-05\n",
      "  1.83592765e-05  7.13899817e-06 -4.02951955e-05 -1.57959475e-05\n",
      "  3.48535723e-05  4.22729681e-05  2.24739906e-05 -3.06834445e-05\n",
      " -9.25902168e-06  1.69705636e-05 -5.59752743e-06  1.87547885e-05\n",
      "  5.66740855e-05 -5.29729596e-06 -2.53772796e-05  4.76818495e-05\n",
      " -2.48118231e-05  1.01198002e-05  3.19436440e-05 -3.08774543e-05\n",
      " -1.90311093e-05  4.40867079e-05 -2.38416055e-06  3.03557536e-05\n",
      " -2.06151617e-05  2.03915370e-05  1.98274009e-07  2.30565256e-05\n",
      "  2.62408066e-05  4.22034054e-06  7.69177350e-05 -4.08093229e-06\n",
      "  1.46632447e-05  1.93185952e-05 -2.08135007e-05  5.20166986e-06\n",
      " -2.55463328e-05  3.36453013e-05 -1.33984304e-05  4.63897595e-05\n",
      " -2.61197420e-05 -9.75127932e-06  2.32237926e-05  2.18983623e-05\n",
      "  4.73453256e-05  1.60010277e-05  4.90620405e-05  4.31090411e-05\n",
      " -8.32052592e-06  1.42334875e-05  9.42750848e-05  2.68606382e-05\n",
      "  2.51849792e-06 -6.28764319e-05 -1.15283556e-05  1.64277171e-05]\n",
      "<langchain_community.vectorstores.chroma.Chroma object at 0x00000220068C0050>\n"
     ]
    }
   ],
   "source": [
    "print(text_representation)\n",
    "print(final_vector)\n",
    "print(vector_chroma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
